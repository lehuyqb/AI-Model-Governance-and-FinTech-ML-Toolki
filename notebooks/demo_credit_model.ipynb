{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# AI Model Governance Toolkit - Explainability Demo\n",
       "\n",
       "This notebook demonstrates the usage of the Explainability Engine for a credit scoring model. We'll:\n",
       "1. Load and prepare sample credit data\n",
       "2. Train a simple credit scoring model\n",
       "3. Use the ModelExplainer to generate global and local explanations"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "import sys\n",
       "sys.path.append('..')\n",
       "\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.ensemble import RandomForestClassifier\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "\n",
       "from explainability.shap_explainer import ModelExplainer"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Load and Prepare Sample Credit Data\n",
       "\n",
       "For this demo, we'll use a synthetic credit scoring dataset. In a real scenario, you would use your actual credit data."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Generate synthetic credit data\n",
       "np.random.seed(42)\n",
       "n_samples = 1000\n",
       "\n",
       "# Generate features\n",
       "data = {\n",
       "    'age': np.random.normal(35, 10, n_samples),\n",
       "    'income': np.random.normal(50000, 20000, n_samples),\n",
       "    'employment_length': np.random.normal(5, 3, n_samples),\n",
       "    'debt_to_income': np.random.normal(0.3, 0.1, n_samples),\n",
       "    'credit_score': np.random.normal(700, 50, n_samples),\n",
       "    'payment_history': np.random.normal(0.95, 0.05, n_samples),\n",
       "    'loan_amount': np.random.normal(10000, 5000, n_samples)\n",
       "}\n",
       "\n",
       "df = pd.DataFrame(data)\n",
       "\n",
       "# Generate target (loan approval) based on features\n",
       "prob = 1 / (1 + np.exp(-(\n",
       "    0.1 * df['credit_score'] +\n",
       "    0.05 * df['income'] -\n",
       "    0.2 * df['debt_to_income'] -\n",
       "    0.1 * df['payment_history']\n",
       ")))\n",
       "df['approved'] = (prob > 0.5).astype(int)\n",
       "\n",
       "# Split data\n",
       "X = df.drop('approved', axis=1)\n",
       "y = df['approved']\n",
       "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
       "\n",
       "# Scale features\n",
       "scaler = StandardScaler()\n",
       "X_train_scaled = scaler.fit_transform(X_train)\n",
       "X_test_scaled = scaler.transform(X_test)\n",
       "\n",
       "print(\"Dataset shape:\", df.shape)\n",
       "print(\"\\nSample of the data:\")\n",
       "display(df.head())\n",
       "print(\"\\nClass distribution:\")\n",
       "display(df['approved'].value_counts(normalize=True))"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Train Credit Scoring Model\n",
       "\n",
       "We'll use a Random Forest classifier as our credit scoring model."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Train model\n",
       "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
       "model.fit(X_train_scaled, y_train)\n",
       "\n",
       "# Evaluate model\n",
       "train_score = model.score(X_train_scaled, y_train)\n",
       "test_score = model.score(X_test_scaled, y_test)\n",
       "\n",
       "print(f\"Training accuracy: {train_score:.3f}\")\n",
       "print(f\"Test accuracy: {test_score:.3f}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Model Explainability\n",
       "\n",
       "Now we'll use our ModelExplainer to understand how the model makes decisions."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Initialize explainer\n",
       "explainer = ModelExplainer(\n",
       "    model=model,\n",
       "    background_data=X_train_scaled,\n",
       "    feature_names=X_train.columns.tolist()\n",
       ")\n",
       "\n",
       "# Get global feature importance\n",
       "importance = explainer.get_feature_importance(n_samples=100, plot=True)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Local Explanations\n",
       "\n",
       "Let's examine how the model made decisions for specific applicants."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Explain a few predictions\n",
       "for i in range(3):\n",
       "    print(f\"\\nApplicant {i+1}:\")\n",
       "    sample = X_test_scaled[i:i+1]\n",
       "    \n",
       "    # Get prediction\n",
       "    pred = model.predict_proba(sample)[0]\n",
       "    print(f\"Prediction probabilities: [Reject: {pred[0]:.3f}, Approve: {pred[1]:.3f}]\")\n",
       "    \n",
       "    # Get local explanation\n",
       "    contributions = explainer.explain_prediction(sample, plot=True)\n",
       "    \n",
       "    # Display feature values\n",
       "    print(\"\\nFeature values:\")\n",
       "    for feature, value in zip(X_test.columns, X_test.iloc[i]):\n",
       "        print(f\"{feature}: {value:.2f}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Regulatory Compliance Report\n",
       "\n",
       "The explainability engine helps ensure regulatory compliance by providing:\n",
       "1. Global feature importance analysis\n",
       "2. Local explanations for individual decisions\n",
       "3. Transparency in model decision-making\n",
       "\n",
       "This information can be used to:\n",
       "- Document model behavior for regulatory requirements\n",
       "- Identify potential bias in the model\n",
       "- Provide explanations to customers when requested\n",
       "- Monitor model stability over time"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }