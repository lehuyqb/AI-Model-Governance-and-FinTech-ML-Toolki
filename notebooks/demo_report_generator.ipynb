{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# AI Model Governance Toolkit - Report Generator Demo\n",
       "\n",
       "This notebook demonstrates how to use the Report Generator module to create comprehensive reports for AI models, including explainability insights, bias analysis, and regulatory compliance information."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "import numpy as np\n",
       "import pandas as pd\n",
       "from sklearn.ensemble import RandomForestClassifier\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "\n",
       "from explainability.shap_explainer import ShapExplainer\n",
       "from bias_detection.fairness_metrics import BiasDetector\n",
       "from report_generator.report_generator import ReportGenerator"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Load and Prepare Sample Credit Data\n",
       "\n",
       "We'll use a synthetic credit dataset with protected attributes for this demo."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Generate synthetic credit data\n",
       "np.random.seed(42)\n",
       "n_samples = 1000\n",
       "\n",
       "# Protected attributes\n",
       "gender = np.random.choice(['M', 'F'], size=n_samples)\n",
       "age = np.random.normal(35, 10, n_samples)\n",
       "age = np.clip(age, 18, 80)\n",
       "\n",
       "# Credit-related features\n",
       "income = np.random.normal(50000, 20000, n_samples)\n",
       "income = np.clip(income, 20000, 150000)\n",
       "credit_score = np.random.normal(700, 50, n_samples)\n",
       "credit_score = np.clip(credit_score, 300, 850)\n",
       "debt_ratio = np.random.normal(0.3, 0.1, n_samples)\n",
       "debt_ratio = np.clip(debt_ratio, 0, 1)\n",
       "\n",
       "# Create DataFrame\n",
       "data = {\n",
       "    'gender': gender,\n",
       "    'age': age,\n",
       "    'income': income,\n",
       "    'credit_score': credit_score,\n",
       "    'debt_ratio': debt_ratio\n",
       "}\n",
       "df = pd.DataFrame(data)\n",
       "\n",
       "# Generate target variable (loan approval) with some bias based on gender\n",
       "approval_prob = (\n",
       "    0.7 * (df['credit_score'] - 300) / 550 +\n",
       "    0.2 * (df['income'] - 20000) / 130000 +\n",
       "    0.1 * (1 - df['debt_ratio']) +\n",
       "    0.1 * (df['gender'] == 'M')  # Introduce bias\n",
       ")\n",
       "approval_prob = np.clip(approval_prob, 0, 1)\n",
       "df['loan_approved'] = (np.random.random(n_samples) < approval_prob).astype(int)\n",
       "\n",
       "# Split data\n",
       "X = df.drop('loan_approved', axis=1)\n",
       "y = df['loan_approved']\n",
       "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
       "\n",
       "# Scale numerical features\n",
       "scaler = StandardScaler()\n",
       "numerical_features = ['age', 'income', 'credit_score', 'debt_ratio']\n",
       "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
       "X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
       "\n",
       "print(\"Data shape:\", df.shape)\n",
       "print(\"\\nSample data:\")\n",
       "df.head()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Train a Credit Scoring Model\n",
       "\n",
       "We'll train a Random Forest model for credit scoring."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Train Random Forest model\n",
       "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
       "model.fit(X_train, y_train)\n",
       "\n",
       "# Evaluate model\n",
       "y_pred = model.predict(X_test)\n",
       "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
       "\n",
       "print(\"Model Performance:\")\n",
       "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
       "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
       "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
       "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
       "print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Generate Model Explainability Insights\n",
       "\n",
       "We'll use the ShapExplainer to generate feature importance and SHAP values."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Initialize ShapExplainer\n",
       "explainer = ShapExplainer(model)\n",
       "\n",
       "# Generate feature importance\n",
       "feature_importance = explainer.explain_feature_importance(X_train)\n",
       "print(\"\\nFeature Importance:\")\n",
       "for feature, importance in feature_importance.items():\n",
       "    print(f\"{feature}: {importance:.4f}\")\n",
       "\n",
       "# Generate SHAP values for a sample\n",
       "sample_idx = 0\n",
       "shap_values = explainer.explain_prediction(X_test.iloc[sample_idx:sample_idx+1])\n",
       "print(\"\\nSHAP Values for Sample:\")\n",
       "for feature, value in shap_values.items():\n",
       "    print(f\"{feature}: {value:.4f}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Perform Bias Analysis\n",
       "\n",
       "We'll use the BiasDetector to analyze potential bias in the model."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Initialize BiasDetector\n",
       "protected_attributes = ['gender']\n",
       "privileged_groups = {'gender': 'M'}\n",
       "bias_detector = BiasDetector(model, protected_attributes, privileged_groups)\n",
       "\n",
       "# Generate bias report\n",
       "bias_report = bias_detector.generate_bias_report(X_test, y_test)\n",
       "print(\"\\nBias Report:\")\n",
       "for metric, values in bias_report.items():\n",
       "    print(f\"\\n{metric}:\")\n",
       "    for attr, value in values.items():\n",
       "        print(f\"  {attr}: {value:.4f}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Generate Comprehensive Report\n",
       "\n",
       "Now we'll use the ReportGenerator to create a comprehensive report combining all the insights."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Initialize ReportGenerator\n",
       "report_gen = ReportGenerator(\n",
       "    model_info={\n",
       "        'name': 'Credit Scoring Model',\n",
       "        'version': '1.0.0',\n",
       "        'type': 'Random Forest Classifier'\n",
       "    }\n",
       ")\n",
       "\n",
       "# Add explainability data\n",
       "report_gen.add_explainability_data(\n",
       "    feature_importance=feature_importance,\n",
       "    shap_values=shap_values,\n",
       "    has_shap_values=True,\n",
       "    has_local_explanations=True\n",
       ")\n",
       "\n",
       "# Add bias analysis data\n",
       "report_gen.add_bias_analysis_data(\n",
       "    bias_report=bias_report,\n",
       "    protected_attributes=protected_attributes,\n",
       "    privileged_groups=privileged_groups\n",
       ")\n",
       "\n",
       "# Add performance metrics\n",
       "report_gen.add_performance_metrics(\n",
       "    metrics={\n",
       "        'accuracy': accuracy_score(y_test, y_pred),\n",
       "        'precision': precision_score(y_test, y_pred),\n",
       "        'recall': recall_score(y_test, y_pred),\n",
       "        'f1_score': f1_score(y_test, y_pred),\n",
       "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
       "    },\n",
       "    y_true=y_test,\n",
       "    y_pred=y_pred,\n",
       "    y_pred_proba=y_pred_proba\n",
       ")\n",
       "\n",
       "# Add regulatory compliance data\n",
       "report_gen.add_regulatory_compliance(\n",
       "    compliance_data={\n",
       "        'EU AI Act': {\n",
       "            'transparency': True,\n",
       "            'fairness': bias_report['disparate_impact']['gender'] >= 0.8,\n",
       "            'accuracy': accuracy_score(y_test, y_pred) >= 0.7\n",
       "        },\n",
       "        'GDPR': {\n",
       "            'explainability': True,\n",
       "            'fairness': bias_report['demographic_parity']['gender'] <= 0.05,\n",
       "            'accuracy': accuracy_score(y_test, y_pred) >= 0.7\n",
       "        }\n",
       "    },\n",
       "    regulations=['EU AI Act', 'GDPR']\n",
       ")\n",
       "\n",
       "# Add recommendations\n",
       "report_gen.add_recommendations([\n",
       "    {\n",
       "        'title': 'Address Gender Bias',\n",
       "        'description': 'The model shows potential gender bias. Consider retraining with balanced data or applying post-processing techniques.',\n",
       "        'priority': 'High'\n",
       "    },\n",
       "    {\n",
       "        'title': 'Improve Model Performance',\n",
       "        'description': 'While the model performs well, there is room for improvement in precision and recall.',\n",
       "        'priority': 'Medium'\n",
       "    }\n",
       "])\n",
       "\n",
       "# Generate reports in different formats\n",
       "html_report = report_gen.generate_html_report()\n",
       "markdown_report = report_gen.generate_markdown_report()\n",
       "json_report = report_gen.generate_json_report()\n",
       "\n",
       "print(\"Reports generated successfully!\")\n",
       "print(f\"HTML report length: {len(html_report)} characters\")\n",
       "print(f\"Markdown report length: {len(markdown_report)} characters\")\n",
       "print(f\"JSON report length: {len(json_report)} characters\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Save Reports\n",
       "\n",
       "Let's save the generated reports to files."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Save reports\n",
       "with open('credit_scoring_report.html', 'w') as f:\n",
       "    f.write(html_report)\n",
       "\n",
       "with open('credit_scoring_report.md', 'w') as f:\n",
       "    f.write(markdown_report)\n",
       "\n",
       "with open('credit_scoring_report.json', 'w') as f:\n",
       "    f.write(json_report)\n",
       "\n",
       "print(\"Reports saved successfully!\")"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }