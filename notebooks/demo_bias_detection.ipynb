{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# AI Model Governance Toolkit - Bias Detection Demo\n",
       "\n",
       "This notebook demonstrates the usage of the Bias Detection Module for analyzing\n",
       "potential bias in a credit scoring model. We'll:\n",
       "1. Load and prepare sample credit data with protected attributes\n",
       "2. Train a credit scoring model\n",
       "3. Analyze the model for various types of bias\n",
       "4. Generate comprehensive bias reports"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "import sys\n",
       "sys.path.append('..')\n",
       "\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.ensemble import RandomForestClassifier\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "\n",
       "from bias_detection.fairness_metrics import BiasDetector"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Load and Prepare Sample Credit Data\n",
       "\n",
       "We'll create a synthetic dataset that includes protected attributes like gender and age,\n",
       "along with other credit-related features."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Generate synthetic credit data with protected attributes\n",
       "np.random.seed(42)\n",
       "n_samples = 1000\n",
       "\n",
       "# Generate protected attributes\n",
       "gender = np.random.choice(['male', 'female'], size=n_samples, p=[0.5, 0.5])\n",
       "age = np.random.normal(35, 10, n_samples)\n",
       "age = np.clip(age, 18, 80)  # Clip age to reasonable range\n",
       "\n",
       "# Generate other features\n",
       "data = {\n",
       "    'gender': gender,\n",
       "    'age': age,\n",
       "    'income': np.random.normal(50000, 20000, n_samples),\n",
       "    'employment_length': np.random.normal(5, 3, n_samples),\n",
       "    'debt_to_income': np.random.normal(0.3, 0.1, n_samples),\n",
       "    'credit_score': np.random.normal(700, 50, n_samples),\n",
       "    'payment_history': np.random.normal(0.95, 0.05, n_samples),\n",
       "    'loan_amount': np.random.normal(10000, 5000, n_samples)\n",
       "}\n",
       "\n",
       "df = pd.DataFrame(data)\n",
       "\n",
       "# Generate target (loan approval) with some bias\n",
       "prob = 1 / (1 + np.exp(-(\n",
       "    0.1 * df['credit_score'] +\n",
       "    0.05 * df['income'] -\n",
       "    0.2 * df['debt_to_income'] -\n",
       "    0.1 * df['payment_history'] +\n",
       "    0.1 * (df['gender'] == 'male').astype(int)  # Introduce gender bias\n",
       ")))\n",
       "df['approved'] = (prob > 0.5).astype(int)\n",
       "\n",
       "# Split data\n",
       "X = df.drop('approved', axis=1)\n",
       "y = df['approved']\n",
       "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
       "\n",
       "# Scale numerical features\n",
       "scaler = StandardScaler()\n",
       "numerical_features = ['age', 'income', 'employment_length', 'debt_to_income', \n",
       "                     'credit_score', 'payment_history', 'loan_amount']\n",
       "X_train_scaled = X_train.copy()\n",
       "X_test_scaled = X_test.copy()\n",
       "X_train_scaled[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
       "X_test_scaled[numerical_features] = scaler.transform(X_test[numerical_features])\n",
       "\n",
       "print(\"Dataset shape:\", df.shape)\n",
       "print(\"\\nSample of the data:\")\n",
       "display(df.head())\n",
       "print(\"\\nClass distribution:\")\n",
       "display(df['approved'].value_counts(normalize=True))"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Train Credit Scoring Model\n",
       "\n",
       "We'll use a Random Forest classifier as our credit scoring model."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Train model\n",
       "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
       "model.fit(X_train_scaled, y_train)\n",
       "\n",
       "# Evaluate model\n",
       "train_score = model.score(X_train_scaled, y_train)\n",
       "test_score = model.score(X_test_scaled, y_test)\n",
       "\n",
       "print(f\"Training accuracy: {train_score:.3f}\")\n",
       "print(f\"Test accuracy: {test_score:.3f}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Bias Detection Analysis\n",
       "\n",
       "Now we'll use our BiasDetector to analyze potential bias in the model."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Initialize bias detector\n",
       "protected_attributes = ['gender', 'age']\n",
       "privileged_groups = {\n",
       "    'gender': 'male',\n",
       "    'age': 35  # Using mean age as privileged value\n",
       "}\n",
       "\n",
       "detector = BiasDetector(\n",
       "    model=model,\n",
       "    protected_attributes=protected_attributes,\n",
       "    privileged_groups=privileged_groups\n",
       ")\n",
       "\n",
       "# Generate comprehensive bias report\n",
       "report = detector.generate_bias_report(X_test_scaled, y_test)\n",
       "\n",
       "# Display report\n",
       "print(\"Bias Analysis Report:\")\n",
       "print(\"\\n1. Disparate Impact Ratios:\")\n",
       "for attr, ratio in report['disparate_impact'].items():\n",
       "    print(f\"{attr}: {ratio:.3f}\")\n",
       "    print(f\"Interpretation: {'Fair' if 0.8 <= ratio <= 1.2 else 'Potential bias detected'}\")\n",
       "\n",
       "print(\"\\n2. Demographic Parity Differences:\")\n",
       "for attr, diff in report['demographic_parity'].items():\n",
       "    print(f\"{attr}: {diff:.3f}\")\n",
       "    print(f\"Interpretation: {'Fair' if diff < 0.1 else 'Potential bias detected'}\")\n",
       "\n",
       "print(\"\\n3. Equal Opportunity Differences:\")\n",
       "for attr, diff in report['equal_opportunity'].items():\n",
       "    print(f\"{attr}: {diff:.3f}\")\n",
       "    print(f\"Interpretation: {'Fair' if diff < 0.1 else 'Potential bias detected'}\")\n",
       "\n",
       "print(\"\\n4. Feature Correlations with Protected Attributes:\")\n",
       "for attr, correlations in report['feature_correlations'].items():\n",
       "    if correlations:\n",
       "        print(f\"\\n{attr} correlations:\")\n",
       "        for feature, corr in correlations.items():\n",
       "            print(f\"{feature}: {corr:.3f}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Visualize Bias Metrics\n",
       "\n",
       "Let's visualize the bias metrics to better understand the model's behavior."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Plot all bias metrics\n",
       "detector.plot_bias_report(report)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Mitigation Recommendations\n",
       "\n",
       "Based on the bias analysis, here are some recommendations for mitigating bias:\n",
       "\n",
       "1. **Data Collection and Preprocessing**\n",
       "   - Ensure balanced representation of protected groups in training data\n",
       "   - Consider removing or transforming features that strongly correlate with protected attributes\n",
       "\n",
       "2. **Model Training**\n",
       "   - Use fairness-aware algorithms or constraints during training\n",
       "   - Consider reweighting samples to balance protected groups\n",
       "\n",
       "3. **Post-processing**\n",
       "   - Implement threshold adjustment for different protected groups\n",
       "   - Use calibration techniques to ensure equal prediction rates\n",
       "\n",
       "4. **Monitoring and Maintenance**\n",
       "   - Regularly monitor bias metrics on new data\n",
       "   - Implement automated bias detection in production\n",
       "   - Maintain documentation of bias mitigation efforts"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   } 